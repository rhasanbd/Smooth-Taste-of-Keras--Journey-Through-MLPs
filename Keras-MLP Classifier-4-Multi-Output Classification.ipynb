{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi-label Classification with a Multi-Output Model\n",
    "\n",
    "In this notebook we perform a multi-label classification with a multi-output model using Keras. We show how to use the **Functional API** for building an arbitrary architecture (multi-output model)\n",
    "\n",
    "\n",
    "We use the MNIST dataset for experimentation.\n",
    "\n",
    "In this problem, each MNIST image has two labels:\n",
    "- label 1: integer representing the digit\n",
    "- label 2: False/True to represent even/odd\n",
    "\n",
    "Thus, both label 1 and label 2 are multiclass:\n",
    "- label 1: 10 class\n",
    "- label 2: 2 class\n",
    "\n",
    "For each image we need to predict two output probabilities (i.e., probabilities of the digit and even/odd). Thus, we build a multi-output multiclass classifier, or simply a **multi-output classifier**.\n",
    "\n",
    "The Sequential Model of Keras doesn't support building multi-output Artificial Neural Networks (ANNs). We use the **Functional API** to do this. The functional API can handle models with non-linear topology, models with shared layers, and models with multiple inputs or outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Dataset\n",
    "\n",
    "We load the train and test dataset using Keras. \n",
    "\n",
    "Then, we flatten the input images to create 1D array for each image.\n",
    "\n",
    "Finally, scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of Training Samples:  (55000, 784)\n",
      "No. of Training Labels:  (55000,)\n",
      "\n",
      "No. of Validation Samples:  (5000, 784)\n",
      "No. of Validation Labels:  (5000,)\n",
      "\n",
      "No. of Testing Samples:  (10000, 784)\n",
      "No. of Testing Labels:  (10000,)\n",
      "\n",
      "X type:  float64\n",
      "y type:  uint8\n"
     ]
    }
   ],
   "source": [
    "mnist = keras.datasets.mnist\n",
    "\n",
    "(X_train_full, y_train_full), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "# Flatten the features to create 1D array for each image\n",
    "X_train_full = X_train_full.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "\n",
    "\n",
    "# Create validation dataset as well as scale the data\n",
    "X_valid, X_train = X_train_full[:5000] / 255.0, X_train_full[5000:] / 255.0\n",
    "y_valid, y_train = y_train_full[:5000], y_train_full[5000:]\n",
    "X_test = X_test / 255.0\n",
    "\n",
    "print(\"No. of Training Samples: \", X_train.shape)\n",
    "print(\"No. of Training Labels: \", y_train.shape)\n",
    "\n",
    "print(\"\\nNo. of Validation Samples: \", X_valid.shape)\n",
    "print(\"No. of Validation Labels: \", y_valid.shape)\n",
    "\n",
    "print(\"\\nNo. of Testing Samples: \", X_test.shape)\n",
    "print(\"No. of Testing Labels: \", y_test.shape)\n",
    "\n",
    "print(\"\\nX type: \", X_train.dtype)\n",
    "print(\"y type: \", y_train.dtype)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Binary Labels\n",
    "\n",
    "Each target y_train/t_valid/y_test represent the underlying image of a digit using an integer between 0 to 9. This target is used for predicting the digit via 10-class classification.\n",
    "\n",
    "For predicting whether the digit is even or odd we need to create another target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Binarize the Target: Create a new target to determine whether the digit is even or odd\n",
    "y_train_binary_1D = (y_train % 2 == 0)\n",
    "y_test_binary_1D = (y_test % 2 == 0)\n",
    "y_valid_binary_1D = (y_valid % 2 == 0)\n",
    "\n",
    "\n",
    "'''\n",
    "The target \"y_train/t_valid/y_test\" are 1D arrays.\n",
    "For each instance, it has just a target class index (0 or 1).\n",
    "We want to compute one target probability per class for each instance.\n",
    "I.e., each instance should have 2 probabilities for 2 classes.\n",
    "Thus, we need to convert class indices (or sparse labels) to one-hot vector labels. \n",
    "The 1D target \"y\" would be converted to Nx2 matrix (N=number of samples)\n",
    "We do this by using the keras.utils.to_categorical() function. \n",
    "'''\n",
    "\n",
    "y_train_binary = keras.utils.to_categorical(y_train_binary_1D)\n",
    "y_test_binary = keras.utils.to_categorical(y_test_binary_1D)\n",
    "y_valid_binary = keras.utils.to_categorical(y_valid_binary_1D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create The Multi-Output Model using Keras Functional API\n",
    "\n",
    "Creating the multi-output model is straightforward. The Functional API allows us to connect the last hidden layer to two output layers (for multiclass and binaey classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 784)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 300)          235500      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 100)          30100       dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           1010        dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 2)            202         dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 266,812\n",
      "Trainable params: 266,812\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Delete the TensorFlow graph before creating a new model, otherwise memory overflow will occur.\n",
    "'''\n",
    "keras.backend.clear_session()\n",
    "\n",
    "'''\n",
    "To reproduce the same result by the model in each iteration, we use fixed seeds for random number generation. \n",
    "'''\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "\n",
    "'''\n",
    "Create a Functional model. \n",
    "- First Layer (input_): It instantiates an input tensor for buildng the model \n",
    "- Hidden Layers: Dense hidden layer with the ReLU activation function\n",
    "- Output Layer 1: Dense output layer with 10 neurons. Since it's a multi-class classification, we use \"softmax\"  \n",
    "- Output Layer 2: Dense output layer with 2 neurons. Since it's a binary classification, we use \"sigmoid\"  \n",
    "'''\n",
    "\n",
    "input_ = keras.Input(shape=(784,))\n",
    "hidden1 = keras.layers.Dense(300, activation=\"relu\")(input_)\n",
    "hidden2 = keras.layers.Dense(100, activation=\"relu\")(hidden1)\n",
    "output1 = keras.layers.Dense(10, activation=\"softmax\")(hidden2)\n",
    "output2 = keras.layers.Dense(2, activation=\"sigmoid\")(hidden2)\n",
    "\n",
    "# Create a Model by specifying its input and outputs\n",
    "model = keras.models.Model(inputs=[input_], outputs=[output1, output2])\n",
    "\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compile and Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 55000 samples, validate on 5000 samples\n",
      "Epoch 1/100\n",
      "55000/55000 [==============================] - 5s 92us/sample - loss: 0.3844 - dense_2_loss: 0.2542 - dense_3_loss: 0.1302 - dense_2_accuracy: 0.9231 - dense_3_accuracy: 0.9518 - val_loss: 0.1737 - val_dense_2_loss: 0.1118 - val_dense_3_loss: 0.0616 - val_dense_2_accuracy: 0.9678 - val_dense_3_accuracy: 0.9785\n",
      "Epoch 2/100\n",
      "55000/55000 [==============================] - 4s 74us/sample - loss: 0.1567 - dense_2_loss: 0.1024 - dense_3_loss: 0.0543 - dense_2_accuracy: 0.9687 - dense_3_accuracy: 0.9814 - val_loss: 0.1231 - val_dense_2_loss: 0.0837 - val_dense_3_loss: 0.0391 - val_dense_2_accuracy: 0.9770 - val_dense_3_accuracy: 0.9872\n",
      "Epoch 3/100\n",
      "55000/55000 [==============================] - 4s 75us/sample - loss: 0.1079 - dense_2_loss: 0.0699 - dense_3_loss: 0.0379 - dense_2_accuracy: 0.9785 - dense_3_accuracy: 0.9867 - val_loss: 0.1202 - val_dense_2_loss: 0.0770 - val_dense_3_loss: 0.0426 - val_dense_2_accuracy: 0.9772 - val_dense_3_accuracy: 0.9845\n",
      "Epoch 4/100\n",
      "55000/55000 [==============================] - 4s 74us/sample - loss: 0.0771 - dense_2_loss: 0.0494 - dense_3_loss: 0.0276 - dense_2_accuracy: 0.9844 - dense_3_accuracy: 0.9906 - val_loss: 0.1122 - val_dense_2_loss: 0.0762 - val_dense_3_loss: 0.0357 - val_dense_2_accuracy: 0.9772 - val_dense_3_accuracy: 0.9884\n",
      "Epoch 5/100\n",
      "55000/55000 [==============================] - 4s 77us/sample - loss: 0.0583 - dense_2_loss: 0.0372 - dense_3_loss: 0.0211 - dense_2_accuracy: 0.9880 - dense_3_accuracy: 0.9926 - val_loss: 0.1145 - val_dense_2_loss: 0.0738 - val_dense_3_loss: 0.0402 - val_dense_2_accuracy: 0.9790 - val_dense_3_accuracy: 0.9875\n",
      "Epoch 6/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.0412 - dense_2_loss: 0.0256 - dense_3_loss: 0.0155 - dense_2_accuracy: 0.9919 - dense_3_accuracy: 0.9945 - val_loss: 0.1131 - val_dense_2_loss: 0.0775 - val_dense_3_loss: 0.0350 - val_dense_2_accuracy: 0.9812 - val_dense_3_accuracy: 0.9888\n",
      "Epoch 7/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.0325 - dense_2_loss: 0.0205 - dense_3_loss: 0.0119 - dense_2_accuracy: 0.9939 - dense_3_accuracy: 0.9960 - val_loss: 0.1097 - val_dense_2_loss: 0.0724 - val_dense_3_loss: 0.0369 - val_dense_2_accuracy: 0.9800 - val_dense_3_accuracy: 0.9877\n",
      "Epoch 8/100\n",
      "55000/55000 [==============================] - 4s 74us/sample - loss: 0.0218 - dense_2_loss: 0.0140 - dense_3_loss: 0.0078 - dense_2_accuracy: 0.9961 - dense_3_accuracy: 0.9975 - val_loss: 0.1377 - val_dense_2_loss: 0.0852 - val_dense_3_loss: 0.0518 - val_dense_2_accuracy: 0.9802 - val_dense_3_accuracy: 0.9864\n",
      "Epoch 9/100\n",
      "55000/55000 [==============================] - 4s 74us/sample - loss: 0.0195 - dense_2_loss: 0.0119 - dense_3_loss: 0.0076 - dense_2_accuracy: 0.9968 - dense_3_accuracy: 0.9976 - val_loss: 0.1172 - val_dense_2_loss: 0.0761 - val_dense_3_loss: 0.0406 - val_dense_2_accuracy: 0.9820 - val_dense_3_accuracy: 0.9883\n",
      "Epoch 10/100\n",
      "55000/55000 [==============================] - 5s 84us/sample - loss: 0.0115 - dense_2_loss: 0.0069 - dense_3_loss: 0.0047 - dense_2_accuracy: 0.9983 - dense_3_accuracy: 0.9986 - val_loss: 0.1239 - val_dense_2_loss: 0.0831 - val_dense_3_loss: 0.0402 - val_dense_2_accuracy: 0.9808 - val_dense_3_accuracy: 0.9886\n",
      "Epoch 11/100\n",
      "55000/55000 [==============================] - 4s 75us/sample - loss: 0.0085 - dense_2_loss: 0.0053 - dense_3_loss: 0.0031 - dense_2_accuracy: 0.9987 - dense_3_accuracy: 0.9991 - val_loss: 0.1189 - val_dense_2_loss: 0.0795 - val_dense_3_loss: 0.0388 - val_dense_2_accuracy: 0.9808 - val_dense_3_accuracy: 0.9907\n",
      "Epoch 12/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.0045 - dense_2_loss: 0.0028 - dense_3_loss: 0.0017 - dense_2_accuracy: 0.9995 - dense_3_accuracy: 0.9997 - val_loss: 0.1128 - val_dense_2_loss: 0.0742 - val_dense_3_loss: 0.0381 - val_dense_2_accuracy: 0.9826 - val_dense_3_accuracy: 0.9907\n",
      "Epoch 13/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.0022 - dense_2_loss: 0.0014 - dense_3_loss: 7.0895e-04 - dense_2_accuracy: 0.9999 - dense_3_accuracy: 1.0000 - val_loss: 0.1194 - val_dense_2_loss: 0.0787 - val_dense_3_loss: 0.0402 - val_dense_2_accuracy: 0.9828 - val_dense_3_accuracy: 0.9908\n",
      "Epoch 14/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.0014 - dense_2_loss: 9.1155e-04 - dense_3_loss: 5.2561e-04 - dense_2_accuracy: 1.0000 - dense_3_accuracy: 1.0000 - val_loss: 0.1189 - val_dense_2_loss: 0.0780 - val_dense_3_loss: 0.0403 - val_dense_2_accuracy: 0.9838 - val_dense_3_accuracy: 0.9910\n",
      "Epoch 15/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 0.0011 - dense_2_loss: 6.9067e-04 - dense_3_loss: 4.1111e-04 - dense_2_accuracy: 1.0000 - dense_3_accuracy: 1.0000 - val_loss: 0.1183 - val_dense_2_loss: 0.0767 - val_dense_3_loss: 0.0411 - val_dense_2_accuracy: 0.9842 - val_dense_3_accuracy: 0.9915\n",
      "Epoch 16/100\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 9.0650e-04 - dense_2_loss: 5.6602e-04 - dense_3_loss: 3.4043e-04 - dense_2_accuracy: 1.0000 - dense_3_accuracy: 1.0000 - val_loss: 0.1211 - val_dense_2_loss: 0.0785 - val_dense_3_loss: 0.0420 - val_dense_2_accuracy: 0.9840 - val_dense_3_accuracy: 0.9914\n",
      "Epoch 17/100\n",
      "54784/55000 [============================>.] - ETA: 0s - loss: 8.0063e-04 - dense_2_loss: 5.0214e-04 - dense_3_loss: 2.9849e-04 - dense_2_accuracy: 1.0000 - dense_3_accuracy: 1.0000Restoring model weights from the end of the best epoch.\n",
      "55000/55000 [==============================] - 4s 73us/sample - loss: 7.9843e-04 - dense_2_loss: 5.0073e-04 - dense_3_loss: 2.9759e-04 - dense_2_accuracy: 1.0000 - dense_3_accuracy: 1.0000 - val_loss: 0.1237 - val_dense_2_loss: 0.0798 - val_dense_3_loss: 0.0433 - val_dense_2_accuracy: 0.9840 - val_dense_3_accuracy: 0.9913\n",
      "Epoch 00017: early stopping\n",
      "CPU times: user 2min 38s, sys: 25.2 s, total: 3min 3s\n",
      "Wall time: 1min 10s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer=keras.optimizers.SGD(learning_rate=1e-1, momentum=0.1)\n",
    "\n",
    "\n",
    "'''\n",
    "Compile the model.\n",
    "Since we are using two different types of loss functions, we specify those using a list.\n",
    "'''\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"binary_crossentropy\"],\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "\n",
    "# Create a callback object of early stopping\n",
    "early_stopping_cb = keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                  min_delta=0, \n",
    "                                  patience=10, \n",
    "                                  verbose=1, \n",
    "                                  mode='auto',\n",
    "                                  restore_best_weights=True)\n",
    "\n",
    "\n",
    "'''\n",
    "Train the model.\n",
    "We need to specify two types of labels for training and validation using lists.\n",
    "'''\n",
    "history = model.fit(X_train, [y_train, y_train_binary], \n",
    "                    batch_size=32, # batch size 32 is default\n",
    "                    epochs=100,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_valid, [y_valid, y_valid_binary]),\n",
    "                    callbacks=[early_stopping_cb])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "The trained model predicts two types of output (output probability matrices):\n",
    "- An N x 10 matrix for 10 class classification (digit)\n",
    "- An N x 2 matrix for binary classification (even or odd)\n",
    "\n",
    "We use these two predicted matrices to evaluate our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Evaluation:  [0.024959471705039454, 0.014488145, 0.010469379, 0.9962364, 0.9967818]\n",
      "Test Evaluation:  [0.10207103988252347, 0.068993315, 0.032915704, 0.9786, 0.9885]\n",
      "\n",
      "******************** Multiclass Classification ********************************************\n",
      "\n",
      "Multiclass Classification - Train Accuracy:  0.9962364\n",
      "Multiclass Classification - Test Accuracy:  0.9786\n",
      "\n",
      "Multiclass Classification - Train Loss:  0.014488145\n",
      "Multiclass Classification - Test Loss:  0.068993315\n",
      "\n",
      "Test Confusion Matrix (Multiclass):\n",
      "[[ 969    0    1    2    0    0    5    0    2    1]\n",
      " [   0 1128    0    2    0    1    2    0    2    0]\n",
      " [   7    4 1002    5    3    0    2    2    7    0]\n",
      " [   0    0    2  996    0    2    0    1    5    4]\n",
      " [   0    0    3    0  965    0    4    1    2    7]\n",
      " [   2    0    0   16    1  863    2    1    4    3]\n",
      " [   5    2    0    1    6    3  939    0    2    0]\n",
      " [   2    7    8    4    4    0    0  986    7   10]\n",
      " [   1    1    1    2    0    2    4    2  960    1]\n",
      " [   2    2    0    5   13    2    1    1    5  978]]\n",
      "\n",
      "Classification Report (Multiclass):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.99      0.98       980\n",
      "           1       0.99      0.99      0.99      1135\n",
      "           2       0.99      0.97      0.98      1032\n",
      "           3       0.96      0.99      0.98      1010\n",
      "           4       0.97      0.98      0.98       982\n",
      "           5       0.99      0.97      0.98       892\n",
      "           6       0.98      0.98      0.98       958\n",
      "           7       0.99      0.96      0.98      1028\n",
      "           8       0.96      0.99      0.97       974\n",
      "           9       0.97      0.97      0.97      1009\n",
      "\n",
      "    accuracy                           0.98     10000\n",
      "   macro avg       0.98      0.98      0.98     10000\n",
      "weighted avg       0.98      0.98      0.98     10000\n",
      "\n",
      "\n",
      "******************** Binary Classification ********************************************\n",
      "\n",
      "Binary Classification - Train Accuracy:  0.9967818\n",
      "Binary Classification - Test Accuracy:  0.9885\n",
      "\n",
      "Binary Classification - Train Loss:  0.010469379\n",
      "Binary Classification - Test Loss:  0.032915704\n",
      "\n",
      "Test Confusion Matrix (Binary):\n",
      "[[4985   89]\n",
      " [  26 4900]]\n",
      "\n",
      "Classification Report (Binary):\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       False       0.99      0.98      0.99      5074\n",
      "        True       0.98      0.99      0.99      4926\n",
      "\n",
      "    accuracy                           0.99     10000\n",
      "   macro avg       0.99      0.99      0.99     10000\n",
      "weighted avg       0.99      0.99      0.99     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_evaluation = model.evaluate(X_train, [y_train, y_train_binary], verbose=0)\n",
    "test_evaluation = model.evaluate(X_test, [y_test, y_test_binary], verbose=0)\n",
    "\n",
    "print(\"Train Evaluation: \", train_evaluation)\n",
    "print(\"Test Evaluation: \", test_evaluation)\n",
    "\n",
    "\n",
    "train_loss_multiclass = train_evaluation[1]\n",
    "train_loss_binary = train_evaluation[2]\n",
    "train_accuracy_multiclass = train_evaluation[3]\n",
    "train_accuracy_binary = train_evaluation[4]\n",
    "\n",
    "test_loss_multiclass = test_evaluation[1]\n",
    "test_loss_binary = test_evaluation[2]\n",
    "test_accuracy_multiclass = test_evaluation[3]\n",
    "test_accuracy_binary = test_evaluation[4]\n",
    "\n",
    "\n",
    "print(\"\\n******************** Multiclass Classification ********************************************\")\n",
    "\n",
    "\n",
    "print(\"\\nMulticlass Classification - Train Accuracy: \", train_accuracy_multiclass)\n",
    "print(\"Multiclass Classification - Test Accuracy: \", test_accuracy_multiclass)\n",
    "\n",
    "print(\"\\nMulticlass Classification - Train Loss: \", train_loss_multiclass)\n",
    "print(\"Multiclass Classification - Test Loss: \", test_loss_multiclass)\n",
    "\n",
    "\n",
    "# model.predict(X_test) method return 10 probabilities per class for each instance (Dimension Nx10)\n",
    "y_test_predicted = model.predict(X_test)\n",
    "y_test_predicted_multiclass = np.argmax(y_test_predicted[0], axis=1) # get the label/index of the highest probability class\n",
    "y_test_predicted_binary = np.argmax(y_test_predicted[1], axis=1) # get the label/index of the highest probability class\n",
    "\n",
    "\n",
    "\n",
    "# model.predict_classes(X_test) method returns the index (class label) with largest probability (1D array)\n",
    "#y_test_predicted= model.predict_classes(X_test)\n",
    "\n",
    "\n",
    "y_train_predicted = model.predict(X_train)\n",
    "y_train_predicted_multiclass = np.argmax(y_train_predicted[0], axis=1) # get the label/index of the highest probability class\n",
    "y_train_predicted_binary = np.argmax(y_train_predicted[1], axis=1) # get the label/index of the highest probability class\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix (Multiclass):\")\n",
    "print(confusion_matrix(y_test, y_test_predicted_multiclass))\n",
    "\n",
    "print(\"\\nClassification Report (Multiclass):\")\n",
    "print(classification_report(y_test, y_test_predicted_multiclass))\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\n******************** Binary Classification ********************************************\")\n",
    "\n",
    "\n",
    "\n",
    "print(\"\\nBinary Classification - Train Accuracy: \", train_accuracy_binary)\n",
    "print(\"Binary Classification - Test Accuracy: \", test_accuracy_binary)\n",
    "\n",
    "print(\"\\nBinary Classification - Train Loss: \", train_loss_binary)\n",
    "print(\"Binary Classification - Test Loss: \", test_loss_binary)\n",
    "\n",
    "\n",
    "print(\"\\nTest Confusion Matrix (Binary):\")\n",
    "print(confusion_matrix(y_test_binary_1D, y_test_predicted_binary))\n",
    "\n",
    "print(\"\\nClassification Report (Binary):\")\n",
    "print(classification_report(y_test_binary_1D, y_test_predicted_binary))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
